%!TEX root = ../main.tex
\chapter{Introduction}

\section{The Need For Optimal Calibrations}
%At the CMS detector, approximately 1.6 terabits per second of data is generated~\cite{CMScollab}. It is, however, neither feasible nor practical to read out all the events.
At its design luminosity of $\lumi = 10^{34}\percms$, the LHC would generate approximately one billion proton-proton collisions every second. It is, however, neither feasible nor practical to read out, record, reconstruct or analyse all the events. To overcome this, the CMS data acquisition (DAQ) system~\cite{CMS:2008xjf,CMS:2023gfb} runs a two-level trigger system to reduce the data throughput: a hardware-based Level-1 Trigger (L1T) and a software-based High-Level Trigger (HLT)~\cite{CMS:2016ngn}; the former runs in a set of FPGA boards and is designed to select up to 100\kHz of events, whist the latter runs in a computer cluster -- the HLT farm -- and is designed to select $\mathcal{O}(\kHz)$ events. For the expected High-Luminosity LHC (HL-LHC) upgrade for \Phasetwo, changes to the trigger system are anticipated to handle the new experimental conditions, which result in higher luminosity and pile-up (PU) --
$\lumi = 7.5\times10^{34}\percms$
and
$\langle\text{PU}\rangle = 200$ --
and increased data complexity~\cite{Collaboration:2759072, CERN-LHCC-2020-004}.

Once an event is accepted by the trigger system, it is recorded in permanent media and reconstructed. A first pass at reconstruction, the \emph{prompt reconstruction}, happens at the Worldwide LHC Computing Grid (WLCG) Tier-0 located at CERN shortly after the data acquisition. The DAQ and trigger system is designed to fit within constraints related not only to the data readout and collection process itself but also to the prompt reconstruction.

In the current \textit{modus operandi} of the experiment, such constraints include the design of the readout electronics that equip the detector, the processing capacity available at the HLT farm, the bandwidth available at the links connecting the experiment site to Tier-0 and the processing capacity available at Tier-0. These constraints impose substantial restrictions on the number of events that can ultimately be processed and stored for offline physics analysis. Although this strategy forms a strong baseline for the trigger system upgrades, it may lead to interesting phenomena remaining unexplored by the CMS collaboration, especially in searches for physics beyond the standard model~\cite{cms_exotica_summary_plots}.
%It is thus crucial to rethink our data acquisition strategy, which in turn will aid in future new physics searches.
%The use of data scouting has already been implemented at CMS, giving us access to more data. But this data is not fully reconstructed, so of low resolution.
The Next Generation Triggers (NGT) project at CMS aims to supplement the planned upgrades, further enhancing the physics potential of CMS during the high-luminosity era~\cite{NGT_proposal}. The overarching goal is being able to fully reconstruct a larger event rate in real-time, with this reconstruction being of such high quality that it can be directly used for physics analyses.

To achieve this offline-like quality reconstruction, a novel calibration workflow is needed, to which the \textbf{Optimal Calibrations} task (Task 3.4) is dedicated. The main objective of this task is to design a fast calibration workflow that meets or exceeds the resolution Tier-0 provides in the offline reconstruction, whilst automating and optimising the calibration processes. This will also introduce data buffering at the HLT. An initial prototype of this workflow is planned to be deployed within a year. As a first step, a collaboration-internal survey was conducted to get an overview of the already existing calibration workflows. Thanks to the survey, potential candidates for the prototype were also identified.
%In this report, we will present the current calibration workflow(s) at CMS, discuss the identified candidates and integration of our workflow into the DAQ system, and finally provide a conclusion and outlook from the first year of the 3.4 Optimal Calibration Task.

This report is structured as follows:
\begin{itemize}
    \item In the remainder of this chapter, we describe the basic structure of the calibration workflows at CMS (Sec. \ref{sec:CMScalibration}), as well as the CMS conditions database and corresponding data model (Sec. \ref{sec:CMS_CondDB}).
    \item In Chapter~\ref{sec:OpCa_HLT} we describe in more detail the calibration workflows relevant for each sub-detector and identify candidates for the NGT calibration workflow.
    \item In Chapter~\ref{sec:NGT-DAQintegration} we discuss the integration of our workflow into the DAQ system.
    \item In Chapter~\ref{sec:conclusions_and_outlook} we provide a conclusion and an outlook from the first year of our task.
\end{itemize}

%%% Thiago reviewed until here.

\section{CMS Calibration Workflows}\label{sec:CMScalibration}
The CMS detector is composed of various sub-detectors, each with specific tasks and functionalities~\cite{CMS:2008xjf,CMS:2023gfb}.
This makes data acquisition a highly intricate task. Fully reconstructing events measured by the detector requires precise knowledge of the detector conditions at the time of data taking. Numerous factors are to be considered frequently, such as: What is the energy response of a given scintillator in the calorimeters? What are the efficiencies of individual pixels within the tracker? Is the geometry of the subdetector components the same as when it was initially aligned?
What is the exact position of the proton-proton collision? Which detector components do not deliver the desired efficiency anymore? To consistently address these issues, alignment and calibration (AlCa) workflows have been put in place to tackle the various challenges. The workflows provide direct feedback to the reconstruction algorithms to ensure full quality for the data taken. The frequency of condition updates ranges from seconds to months. While some conditions get updated in real time, others may only be updated once a year. The methods to AlCa workflows also vary amongst the subdetectors and their needs.

%\subsection{PCL}
One crucial ingredient to all AlCa workflows is the \texttt{ALCARECO} stream (short for Alignment and Calibration RECO).
This is a specialised data stream designed to support the alignment, calibration, and monitoring of the detector. \texttt{ALCARECO} streams are produced by applying targeted reconstruction and selection steps to the raw data, focusing on events or objects essential for specific calibration and alignment tasks.
By reducing the volume of data to only the relevant subset, \texttt{ALCARECO} streams significantly reduce the computational and storage requirements while maintaining the precision needed for these activities. Examples include streams for muons used in alignment of the tracker and muon systems, electrons for electromagnetic calorimeter calibration, or events rich in hadrons for calorimeter response studies. These streams are integral to maintaining the performance and accuracy of the CMS detector throughout data-taking.

In addition to \texttt{ALCARECO} streams, CMS also employs \texttt{ALCARAW} streams, which are specifically designed for the calibration of the electromagnetic (ECAL) and hadronic (HCAL) calorimeters. \texttt{ALCARAW} streams preserve a subset of raw detector information, enabling highly specialised offline reconstruction algorithms to extract calibration constants with maximum precision.
These streams are optimised to retain only the raw data necessary for calibration, such as information from selected crystals in ECAL or specific HCAL channels, while discarding irrelevant data. By focusing on events like electrons or photons for ECAL, or isolated hadrons for HCAL, \texttt{ALCARAW} streams ensure efficient use of resources while providing high-quality input for calorimeter calibration workflows. This process is crucial for maintaining the energy resolution and response uniformity of the calorimeters over time.

Another central AlCa workflow is the \textbf{Prompt Calibration Loop} (PCL)~\cite{Futyan:2010zz,Cerminara:2015hov}, where conditions are derived prior to full reconstruction. After the event selection through the HLT, there are two reconstruction streams to which we now draw attention to: \textit{Express} and \textit{Physics}. The express stream processes a subset of the triggered data and runs the express reconstruction. The express stream has highest priority at Tier-0 and is reconstructed within several hours (typically below 12). The conditions are derived from this stream and uploaded to the offline conditions database (DB)~\cite{Guida:2015gvw}. There exist additional calibration streams within PCL outside of the express stream, that also contribute to the DB.
The physics stream then undergoes the prompt reconstruction, where it uses the conditions from the DB. This is done within 48 hours and the outcome gets stored and then used for physics analysis. These workflows are visualised within Fig.~\ref{fig:PCL}.

Other calibration workflows exist, such as \textbf{Online to Offline} (O2O). O2O plays a critical role, as it passes on the online conditions within the sub-detector infrastructure to DB, making it accessible for event reconstruction.
% Can we expand on this? There's some details in the tracker (strips) and HCAL section. Therefore, it would be good to have a single section that can be referenced back to.
% It would be useful to add more technical details (e.g. how it uploads the conditions - O2O?)
Another set of workflows go through as automated framework called \textbf{Automation},
based on the Jenkins software and developed for \Runthree~\cite{Pigazzini:2853679}.
This framework is especially useful for workflows that need more than 48 hours to be executed.
It takes the Prompt reconstruction datasets as input and makes use of the accumulated data for derivation of the alignment and calibrations constants that need a sizeable sample. Typical workflows use a few inverse femtobarns of data for the uploads of the conditions to future Prompt reconstruction.

\begin{figure}[h!]	
\centering
\includegraphics[width=\textwidth]{figures/PCL.jpg} %\hfill
\caption{A schematic overview of the current Prompt Calibration Loop (PCL) implemented at CMS, showcasing its integral role to the data reconstruction workflow.} % Source for this? NOTE: Not sure if even AlCa knows the source :D
\label{fig:PCL}
\end{figure}


\section{The CMS Conditions Database} \label{sec:CMS_CondDB}

The CMS condition database (\texttt{CondDB}) is a specialised database system used to store and manage the non event data required for detector operations, simulation, and data analysis. With conditions data we mean non-event data resulting mainly from calibrations and alignment algorithms.

\begin{figure}[h!]	
\centering
\includegraphics[width=0.8\textwidth]{figures/figure_DBArchitecture.png} %\hfill
\caption{CMS condition databases architecture~\cite{DeGruttola:2010gb}.}
\label{fig:CondDB}
\end{figure}

\subsection{Structure of the Conditions Data Model}
The condition data is stored in \emph{payloads} and described with additional metadata information, such as the \emph{Interval of
Validity} (IOV). It is organised into \emph{Tags} and \emph{Global Tags}.
\begin{itemize}
\item The ``atom'' of conditions data is the \emph{Payload}, representing the set of parameters consumed in the workflows of the physics data processing. It is associated to a user-defined C++ class in the CMS offline reconstruction software \texttt{CMSSW}~\cite{CMSSW_ref} (\texttt{CondFormats} objects)
\item The time information for the validity of the \emph{Payloads} is specified with a parameter called \emph{Interval Of Validity} (IOV). Time is represented by a Run number, luminosity section id, or an universal timestamp.
\item A \emph{Tag} is a fully qualified set of conditions consisting of
\emph{Payloads} and their associate IOVs. They should cover the time span required by the workload.
\item In \texttt{CMSSW}, a special C++ class (the \emph{Record}) acts as entry point for a \emph{Payload} by importing database content on the \texttt{cmsRun} executable. Every processing workflow consumes a specific set of \emph{Records}.
\item A \emph{Global Tag} (often abbreviated GT in the following) is a set of (\emph{Record}, \emph{Tag}) pairs needed by a particular processing workflow (online data-taking, MC, re-processing). This is the final ``product'', typically consumed by CMS users.
\end{itemize}

There are currently 326 conditions (records/tags) in the latest HLT Global Tag (\texttt{140X\_dataRun3\_HLT\_v3}) used during LHC \Runthree pp data taking in 2024.
The authors of this report conducted a survey among the CMS Detector Performance Groups to evaluate the Global Tag's content, focusing on the types of workflows used to populate these conditions and the criticality of their updates.
A copy of the survey is available in\\
\url{https://cern.ch/ngt-cms-optimalcalibrations-survey2024}.

% NOTE: Introduce prompt/offline?
