%!TEX root = ../main.tex
\chapter{Introduction}

\section{The Need For Optimal Calibrations}
%At the CMS detector, approximately 1.6 terabits per second of data is generated \cite{CMScollab}. It is, however, neither feasible nor practical to read out all the events. 
At its design luminosity of $\lumi = 10^{34}\percms$, the LHC would generate approximately one billion proton-proton collisions every second. It is, however, neither feasible nor practical to read out, record, reconstruct or analyse all the events. To overcome this, the CMS data acquisition (DAQ) system runs a two-level trigger system to reduce the data throughput: a hardware-based Level-1 Trigger (L1T) and a software-based High-Level Trigger (HLT)~\cite{CMS:2016ngn}; the former runs in a set of FPGA boards and is designed to select up to 100\kHz of events, whist the latter runs in a computer cluster -- the HLT farm -- and is designed to select $\mathcal{O}\kHz$ events. For the expected High-Luminosity LHC (HL-LHC) upgrade, changes to the trigger system are anticipated to handle the new experimental conditions, which result in higher luminosity and pile-up (PU) and increased data complexity~\cite{Collaboration:2759072, CERN-LHCC-2020-004}. 

Once an event is accepted by the trigger system, it is recorded in permanent media and reconstructed. A first pass at reconstruction, the \emph{prompt reconstruction}, happens at the WLCG Tier-0 located at CERN shortly after the data acquisition. The DAQ and trigger system is designed to fit within constraints related not only to the data readout and collection process itself but also to the prompt reconstruction.

In the current \textit{modus operandi} of the experiment, such constraints include the design of the readout electronics that equip the detector, the processing capacity available at the HLT farm, the bandwidth available at the links connecting the experiment site to Tier-0 and the processing capacity available at Tier-0. These constraints impose substantial restrictions on the number of events that can ultimately be processed and stored for offline physics analysis. Although this strategy is successful, and remains the baseline for the trigger system upgrades, it may lead to interesting phenomena remaining unexplored by the CMS collaboration, especially in searches for physics beyond the standard model~\cite{cms_exotica_summary_plots}.
%It is thus crucial to rethink our data acquisition strategy, which in turn will aid in future new physics searches. 
%The use of data scouting has already been implemented at CMS, giving us access to more data. But this data is not fully reconstructed, so of low resolution. 
The Next Generation Trigger (NGT) project at CMS aims to supplement the planned upgrades, further enhancing the physics potential of CMS during the high-luminosity era. The overarching goal is being able to fully reconstruct a larger event rate in real-time, with this reconstruction being of such high quality that it can be directly used for physics analyses.

To achieve this goal, a novel calibration workflow is needed, to which the \textbf{Optimal Calibrations} task (Task 3.4) is dedicated. The main objective of this task is to design a fast calibration workflow that meets or exceeds the resolution Tier-0 provides in the offline reconstruction, whist automating and optimising the calibration processes. This will also introduce data buffering at the HLT and a prototype of this workflow is planned to be deployed within a year. As a first step, task members acquainted themselves with the existing calibration workflows and selected ideal candidates for the prototype, thanks to a collaboration-internal survey. 

%In this report, we will present the current calibration workflow(s) at CMS, discuss the identified candidates and integration of our workflow into the DAQ system, and finally provide a conclusion and outlook from the first year of the 3.4 Optimal Calibration Task.
This report is structured as follows.
In the remainder of this chapter, we describe the basic structure of the calibration workflows at CMS, as well as the CMS conditions database and corresponding data model.
In Chapter 2 we describe in more detail the calibration workflows relevant for each sub-detector and identify candidates for the NGT calibration workflow.
In Chapter 3 we discuss the integration of our workflow into the DAQ system.
In Chapter 4 we provide a conclusion and and outlook from the first year of our task.

%%% Thiago reviewed until here.

\section{CMS Calibration Workflows}\label{sec:CMScalibration}
The CMS detector is composed of various sub-detectors, each with specific tasks and functionalities. This makes data acquisition a highly intricate task. The main calibration workflows happen at Tier-0. Fully reconstructing events measured by the detector requires precise knowledge of the detector conditions at the time of data taking. Numerous factors are to be considered frequently, such as: What is the energy response of a given scintillator in the calorimeters? What are the efficiencies of individual pixels within the tracker? Is everything still exactly located where it was initially placed? What is the exact position of the proton-proton collision? Which detector components do not deliver the desired efficiency anymore? To consistently address these issues, a calibration and alignment (AlCa) workflow has been put in place to tackle the various challenges. The workflow provides direct feedback to the reconstruction algorithms to ensure full quality for the data taken. The frequency of condition updates ranges from seconds to months. While some conditions get updated in real-time, others may only be updated once a year. The methods to AlCa workflows also vary amongst the subdetectors and their needs.

%\subsection{PCL}
\newline \newline
Central to the AlCa workflow is the \textbf{Prompt Calibration Loop} (PCL) \cite{David_Futyan_2010,Cerminara_2015}, where conditions are derived prior to full reconstruction. After the HLT, there are two main reconstruction streams: \textit{Express} and \textit{Physics}. The express stream processes a subset of the triggered data and runs the express reconstruction. The express stream has highest priority at Tier-0 and is completed within 1-2 hours. The conditions are derived from this stream and uploaded to the offline conditions database (DB) \cite{di2015cms}. There exist additional calibration streams within PCL outside of the express stream, that also contribute to the DB.
The physics stream then undergoes the prompt reconstruction, where it uses the conditions from the DB. This is done within 48 hours and the outcome gets stored and then used for physics analysis. These workflows are visualised within Figure \ref{fig:PCL}. 

Other calibration workflows exist, such as \textbf{Online to Offline} (O2O). O2O plays a critical role, as it passes on the online conditions within the sub-detector infrastructure to DB, making it accessible for event reconstruction.
\newline \newline

For workflows that need more than 48 hours to be executed, an automated framework called \textbf{Automation} \cite{Pigazzini:2853679}, Jenkins based, was developed for Run 3. It takes the Prompt reconstruction datasets as input and makes use of the accumulated data for derivation of the alignment and calibrations constants that need a sizeable sample. Typical workflows use a few inverse femtobarns of data for the uploads of the conditions to future Prompt reconstruction.  

\begin{figure}[h!]	
\centering
\includegraphics[width=\textwidth]{figures/PCL.jpg} %\hfill
\caption{A schematic overview of the current Prompt Calibration Loop (PCL) implemented at CMS, showcasing its integral role to the data reconstruction workflow.} % Source for this? NOTE: Not sure if even AlCa knows the source :D 
\label{fig:PCL}
\end{figure}

\section{The CMS Conditions Database}

The CMS condition database (\texttt{CondDB}) is a specialized database system used to store and manage the non event data required for detector operations, simulation, and data analysis. With conditions data we mean non-event data resulting mainly from calibrations and alignment algorithms.

\begin{figure}[h!]	
\centering
\includegraphics[width=0.8\textwidth]{figures/figure_DBArchitecture.png} %\hfill
\caption{CMS condition databases architecture. \cite{Gruttola_2010}} 
\label{fig:CondDB}
\end{figure}

\subsection{Structure of the Conditions Data Model}
The condition data is stored in \emph{payloads} and described with additional metadata information, such as the \emph{Interval of
Validity} (IOV). It is organized into \emph{Tags} and \emph{Global Tags}.
\begin{itemize}
\item The "atom‚Äù of conditions data is the \emph{Payload}, representing the set of parameters consumed in the workflows of the physics data processing. It is associated to a user-defined C++ class in the CMS offline reconstruction software \texttt{CMSSW} (\texttt{CondFormats} objects)
\item The time information for the validity of the \emph{Payloads} is specified with a parameter called \emph{Interval Of Validity} (IOV). Time is represented by a Run number, luminosity section id, or an universal timestamp.
\item A \emph{Tag} is a fully qualified set of conditions consisting of
\emph{Payloads} and their associate IOVs. They should cover the time span required by the workload. 
\item In \texttt{CMSSW}, a special C++ class (the \emph{Record}) acts as entry point for a \emph{Payload} by importing database content on the \texttt{cmsRun} executable. Every processing workflow consumes a specific set of \emph{Records}.
\item A \emph{Global Tag} (often abbreviated GT in the following) is a set of (\emph{Record}, \emph{Tag}) pairs needed by a particular processing workflow (online data-taking, MC, re-processing). This is the final "product", typically consumed by CMS users.
\end{itemize}

%There are currently 326 conditions (records/tags) in the latest HLT Global Tag (\texttt{140X\_dataRun3\_HLT\_v3}) used during LHC Run 3 p-p dahttps://github.com/rovere/NGT_Task3.1.1_Report_Year1/tree/main/chaptersta taking in 2024.

% NOTE: Introduce prompt/offline?